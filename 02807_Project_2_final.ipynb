{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlsPJUw7wCWZ"
   },
   "source": [
    "# 02807: Project 2\n",
    " \n",
    "## Practical information\n",
    " \n",
    "* This project must be completed in groups of 3 students.\n",
    "    * The group must be registered on the course site on DTU Learn: My Course > Groups\n",
    "    * Groups must be registered anew (even if you already registered for Project 1)\n",
    "* This project must be handed in as a jupyter notebook to the course site on DTU Learn. \n",
    "    * Go to the Course Content > Assignments tab to upload your submission. \n",
    "* This project is due on Monday, November 29, 20:00.\n",
    "\n",
    "## Submission rules\n",
    "\n",
    "* Each group has to hand in *one* notebook (`.ipynb`) with their solutions, including a filled out Contribution table (see below).\n",
    "* Your solution must be written in Python.\n",
    "* For each question you should use the cells provided (\"`# your code goes here`\" and \"*your explanation here*\") for your solution\n",
    "    * It is allowed to add code cells within a question block, but consider if it's really necessary.\n",
    "* You should not remove the problem statements, and you should not modify the structure of the notebook.\n",
    "* Your notebook should be runnable and readable from top to bottom.\n",
    "    * Meaning that your code cells work when run in order (from top to bottom).\n",
    "    * Output of any cell depends only on itself and cells above it.\n",
    "* Your notebook should be submitted after having been run from top to bottom.\n",
    "    * This means outputs are interpretable without necessarily running your cells.\n",
    "    * The simplest way to achieve this is using the jupyter menu item Kernel > Restart & Run All just prior to submission. If any cell fails when you do this, your notebook is not ready for submission.\n",
    "    * Exercise 3 in particular will take time to finish, plan accordingly, that is, make sure you have time to run your notebook from top to bottom.\n",
    "* Failure to comply may make it impossible for us to evaluate your submission properly, which will likely negatively impact the points awarded.\n",
    "\n",
    "## Solution guidelines\n",
    "* Data processing is via Spark for the first three exercises and pandas/SQL in the fourth exercise.\n",
    "* Where naming of dataframes and functions are explicitly stated, these must be used.\n",
    "* Your solutions will be evaluated by correctness, code quality and interpretability of the output. \n",
    "    * You have to write clean, readable and efficient Spark code that will generate sensible execution plans.\n",
    "    * You have to write clean, readable and efficient SQL queries.\n",
    "    * Your tables and visualisations should be meaningful and easy to read. This requires, but is not limited to, including headers, legends and well-written (brief) descriptions for graphs/charts. In this step you've found the data processing solution, so put also some effort into its presentation.\n",
    "\n",
    "## Colaboration policy\n",
    " \n",
    "* It is not allowed to collaborate on the exercises with students outside your group, except for discussing the text of the exercise with teachers and fellow students enrolled on the course in the same semester. \n",
    "* It is not allowed to exchange, hand-over or in any other way communicate solutions or parts of solutions to the exercises. \n",
    "* It is not allowed to use solutions from similar courses, or solutions found elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZP30rwxHDQyG"
   },
   "source": [
    "## Contribution table and grading\n",
    "\n",
    "* The total amount of points in the project is 150.\n",
    "* You have to indicate who has solved each part of each exercise in a **contribution table**. \n",
    "* A group member can take credit for solving a part of an exercise only if they have contributed **substantially** to the solution. \n",
    "    * Simple contributions, such as correcting a small bug or double-checking the results of functions, are not sufficient for taking credit for a solution.\n",
    "    * Several group members can take credit for the same solution if they all have contributed substantially to it.\n",
    "* Each group member must contribute **at least 65 points**. \n",
    "    * If no name is provided for an exercise's part, **all group members** are considered contributors to it.\n",
    "* Group members should decide amongst themselves how to collaborate on the project to meet these constraints.  \n",
    "* Scores are individual. The score $\\text{score}(m)$ for a group member $m$ ranges from 0 to 10 and is calculated as follows: \n",
    "\n",
    "  * $\\text{individual-score}(m) = \\frac{\\text{total number of points for the parts correctly solved by }m}{\\text{total number of points for the parts contributed by }m}$\n",
    "\n",
    "  * $\\text{group-score} = \\frac{\\text{total number of points correctly solved by any group member}}{\\text{total number of points in the project}}$\n",
    "\n",
    "  * $\\text{score}(m) =  7.5 \\cdot \\text{individual-score}(m) + 2.5 \\cdot \\text{group-score}$\n",
    "  \n",
    "  \n",
    "* The following is an example of a contributions table:\n",
    "\n",
    "|        | Exercise 1 | Exercise 2 | Exercise 3 | Exercise 4 |\n",
    "|--------|------------|------------|------------|------------|\n",
    "| **Part 1** | John       |    Mary        |     Ann       |   Mary, Ann         |\n",
    "| **Part 2** |     Mary       |    Mary        |   Ann         |    John, Ann        |\n",
    "| **Part 3** |     John, Mary, Ann       |      John, Ann      |   John         | John      |\n",
    "| **Part 4** | Ann       |  Ann          |     John, Mary       | John       |\n",
    "| **Part 5** | **n.a.**     | John, Mary, Ann           | **n.a.**       | **n.a.**       |\n",
    "\n",
    "\n",
    "* **Example**: in the contribution table above, suppose that all parts are solved correctly except for those of Exercise 4 which are all wrong. Then Ann's score is calculated as follows:\n",
    "\n",
    "  * $\\text{individual-score}(Ann) = \\frac{5+5+10+5+5+15+15}{5+5+10+5+5+15+15+15+5} = \\frac{60}{80} = 0.75$\n",
    "\n",
    "  * $\\text{group-score} = \\frac{95}{150} = 0.633$\n",
    "\n",
    "  * $\\text{score}(Ann) = 7.5\\cdot 0.75 + 2.5 \\cdot 0.633 = 7.21$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vklet8XdRGVV"
   },
   "source": [
    "# Group contribution table \n",
    "\n",
    "This table must be filled before submission.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T13:07:49.385821Z",
     "start_time": "2021-11-02T13:07:48.995596Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "chiXA3CzRSA1",
    "outputId": "752ba847-32fb-48e7-a4ea-2ce363e3a706"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "d = {'Exercise 1' : ['', '', '', '', 'n.a'], \n",
    "     'Exercise 2' : ['', '', '', '', ''],\n",
    "     'Exercise 3' : ['', '', '', '', 'n.a'],\n",
    "     'Exercise 4' : ['', '', '', '', 'n.a'],\n",
    "     } \n",
    "  \n",
    "ct = pd.DataFrame(d, index=['Part 1', 'Part 2', 'Part 3', 'Part 4', 'Part 5']) \n",
    "\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPlNQTICo9sH"
   },
   "source": [
    "# The AirBnB dataset\n",
    "\n",
    "<img src=\"https://www.esquireme.com/public/images/2019/11/03/airbnb-678x381.jpg\" alt=\"airbnb\" width=\"400\"/>\n",
    "\n",
    "[Airbnb](http://airbnb.com) is an online marketplace for arranging or offering lodgings. In the first three exercises you will use Spark to analyze data obtained from the Airbnb website (stricly speaking via data scraped by [insideairbnb](http://insideairbnb.com/get-the-data.html)). The purpose of your analysis is to extract insights about listings as a whole, specifics about London, and sentiment analysis of reviews (word positivity).\n",
    "\n",
    "\n",
    "## Loading data\n",
    "The dataset consists of listings (offered lodgings) and reviews (submitted by users). The `.csv`'s you'll work with vary between the first three exercises, but is structured so that the function below will load it into a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T13:07:51.283648Z",
     "start_time": "2021-11-02T13:07:51.280806Z"
    },
    "id": "E9BEj7re4jnN"
   },
   "outputs": [],
   "source": [
    "def load_csv_as_dataframe(path):\n",
    "    return spark.read.option('header', True) \\\n",
    "                .option('inferSchema', True) \\\n",
    "                .option('multiLine', 'True') \\\n",
    "                .option('escape', '\"') \\\n",
    "                .option('mode', 'DROPMALFORMED')\\\n",
    "                .csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4hk-43Io9sI"
   },
   "source": [
    "## Imports and Spark session\n",
    "\n",
    "* You'll need to adapt the `JAVA_HOME` environment variable to your setup. \n",
    "* You should set the `spark.driver.memory` value to the amount of memory on your machine. \n",
    "* It may be required for you to install some of the packages imported below (e.g. pandasql)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T13:32:31.290655Z",
     "start_time": "2021-11-02T13:32:31.284171Z"
    },
    "id": "Zy1rMfRh4jnN"
   },
   "outputs": [],
   "source": [
    "# Instructions on p. 20 Learning Spark, 2nd ed.\n",
    "# Here's a quick-guide, googling may also be required\n",
    "# 1) Install pyspark via conda/pip\n",
    "#          pyspark requires the JAVA_HOME environment variable is set.\n",
    "# 2) Install JDK 8 or 11, figure out the install location\n",
    "#          Suggest to use https://adoptopenjdk.net/\n",
    "# 3) Update the JAVA_HOME environment variable set programmatically below \n",
    "#    with your install location specifics\n",
    "\n",
    "# JAVA_HOME environment variable is set programatically below\n",
    "# but you must point it to your local install\n",
    "\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
    "\n",
    "# If you get \"Job aborted due to stage failure\" and \n",
    "# \"Python worker failed to connect back.\" exceptions, \n",
    "# this should be solved by additionally setting these \n",
    "# environment variables\n",
    "\n",
    "# os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'notebook'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T13:07:53.807655Z",
     "start_time": "2021-11-02T13:07:52.901610Z"
    },
    "id": "2ftW7yaGo9sJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pyspark\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pandas_profiling import ProfileReport\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import countDistinct\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "import pandasql as psql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T13:07:57.883383Z",
     "start_time": "2021-11-02T13:07:53.808476Z"
    },
    "id": "OBILY9sf4jnN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/29 18:08:09 WARN Utils: Your hostname, T-XPS-15-9500 resolves to a loopback address: 127.0.1.1; using 10.209.225.210 instead (on interface wlp0s20f3)\n",
      "21/11/29 18:08:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/11/29 18:08:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Sets memory limit on driver and to use all CPU cores\n",
    "conf = SparkConf().set('spark.ui.port', '4050') \\\n",
    "        .set('spark.driver.memory', '4g') \\\n",
    "        .setMaster('local[*]')\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T13:07:58.627425Z",
     "start_time": "2021-11-02T13:07:57.884329Z"
    },
    "id": "cJpOoxzN4jnN",
    "outputId": "517231b3-51f5-4e7f-9481-8384d0989a00"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://t-xps-15-9500.students.clients.local:4050\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9c4e0ed310>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T13:07:58.667031Z",
     "start_time": "2021-11-02T13:07:58.628174Z"
    },
    "id": "L5lJlshk4jnN",
    "outputId": "95c2c9ea-e61f-4ac1-ab01-14b494a549c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.startTime', '1638205690399'),\n",
       " ('spark.driver.host', 't-xps-15-9500.students.clients.local'),\n",
       " ('spark.driver.memory', '4g'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.ui.port', '4050'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/home/thomas/jupyter/ComputationalToolsAss2/spark-warehouse'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.id', 'local-1638205690955'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.port', '38121')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PdveMrU4jnO"
   },
   "source": [
    "# Exercise 1: Listings and cities (20 pts)\n",
    "\n",
    "In this exercise you must use Spark to do the data processing. \n",
    "* For parts where you present tabular data, this entails calling `toPandas` as the final step of your query. \n",
    "* For parts requiring visualisation, the `toPandas` call should be followed only by functions necessary to customize the plotting/layout steps (i.e. no data processing take place after your spark dataframe is materialized).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5LAuprs4jnO"
   },
   "source": [
    "## Part 1: Preparing the dataframe (5 pts)\n",
    "\n",
    "Your data source is [this zip archive](https://data-download.compute.dtu.dk/c02807/listings.csv.zip) which you must uncompress and place in the same folder as this notebook. It is loaded in the next cell and named `df_listings`.\n",
    "\n",
    "After the data is read, you should select the columns necessary for exercise 1, 2 and 3 (by reading ahead or iteratively extend this loading code). Name this dataframe `df_listings_analysis` and make use of caching.\n",
    "\n",
    "Prices are in local currency, but are nonetheless prefixed with `$` and contains thousands separator commas. You will need to remove these characters and cast the price column to `pyspark.sql.types.DoubleType`. Observe that if this casting is not possible, the result of the cast is `null`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T15:29:07.681609Z",
     "start_time": "2021-10-27T15:28:34.756446Z"
    },
    "id": "NTybzL654jnO"
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/home/thomas/jupyter/ComputationalToolsAss2/listings.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_165320/312211064.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_listings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csv_as_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'listings.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_165320/1673199834.py\u001b[0m in \u001b[0;36mload_csv_as_dataframe\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_csv_as_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'header'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inferSchema'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multiLine'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'True'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'escape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/jupyter_env/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/jupyter_env/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/jupyter_env/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/home/thomas/jupyter/ComputationalToolsAss2/listings.csv"
     ]
    }
   ],
   "source": [
    "df_listings = load_csv_as_dataframe('listings.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['id', 'city','neighbourhood_cleansed', 'price', 'review_scores_rating', 'property_type']\n",
    "df_listings_analysis = df_listings.select(*cols).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_analysis = df_listings_analysis.withColumn('price', regexp_replace(col(\"price\"), \"[\\$,]\", \"\"))\n",
    "df_listings_analysis = df_listings_analysis.withColumn(\"price\",df_listings_analysis.price.cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_analysis.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QZ0w17Y4jnO"
   },
   "source": [
    "## Part 2: Listing and neighbourhood counts (5 pts)\n",
    "\n",
    "Compute and visualise the number of listings and the number of different neighbourhoods per city, restricted to the 15 cities having the most listings. The x-axis should be ordered by number of listings (high to low).\n",
    "\n",
    "Make sure to use the `neighbourhood_cleansed` column in your computations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:35:44.882818Z",
     "start_time": "2021-10-31T14:35:44.880268Z"
    },
    "id": "kxo1ZH_q4jnO"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "df_pd = df_listings_analysis.groupBy(\"city\") \\\n",
    "    .agg(F.count('city').alias('listings'), F.countDistinct('neighbourhood_cleansed').alias('distinct_neighbourhood')) \\\n",
    "    .orderBy('listings', ascending=False).limit(15).toPandas()\\\n",
    "    #.plot(kind=\"bar\", x='city', y=['listings','distinct_neighbourhood'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(13,5))\n",
    "_ = df_pd.plot(ax=axes[0], kind=\"bar\", x='city', y='listings')\n",
    "_ =df_pd.plot(ax=axes[1], kind=\"bar\", x='city', y='distinct_neighbourhood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9BFleEJ4jnO"
   },
   "source": [
    "## Part 3: Price averages (5 pts)\n",
    "\n",
    "Compute and visualise the average price of listings per city, restricted to the 15 cities having the most listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:35:50.717376Z",
     "start_time": "2021-10-31T14:35:50.714248Z"
    },
    "id": "nfggw4Sp4jnO"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "_ = df_listings_analysis.groupBy(\"city\") \\\n",
    "    .agg(F.avg('price').alias('avgPrice'), F.count('city').alias('listings'))\\\n",
    "    .orderBy('listings', ascending=False).limit(15) \\\n",
    "    .toPandas().plot(kind=\"bar\", x='city', y='avgPrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIpHBUBk4jnO"
   },
   "source": [
    "## Part 4: Value for money (5 pts)\n",
    "\n",
    "The value of a listing is its rating divided by its price. The value of a city is the average value of its listings. \n",
    "\n",
    "Prices are only comparable when the local currency is the same. We'll therefore consider a subset of Euro-zone cities as defined in `eurozone_cities`.\n",
    "\n",
    "Compute and visualise the value per city, restricted to the Euro-zone cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T15:29:53.566725Z",
     "start_time": "2021-10-27T15:29:53.565232Z"
    },
    "id": "wqspMVZ24jnO"
   },
   "outputs": [],
   "source": [
    "eurozone_cities = [\n",
    "    'Paris', 'Roma', 'Berlin', 'Madrid', 'Amsterdam', 'Barcelona', 'Milano', 'Lisboa',\n",
    "    'München', 'Wien', 'Lyon', 'Firenze', 'Porto', 'Napoli', 'Bordeaux', 'Venezia',\n",
    "    'Málaga', 'Sevilla', 'València'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:35:57.257011Z",
     "start_time": "2021-10-31T14:35:57.253676Z"
    },
    "id": "YtktiltR4jnP"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "_ = df_listings_analysis.filter(df_listings_analysis.city.isin(eurozone_cities))\\\n",
    "            .withColumn('value_listing',F.col('review_scores_rating')/F.col('price')) \\\n",
    "            .groupBy('city').agg(F.avg('value_listing').alias('avgRating')).toPandas() \\\n",
    "            .plot(kind=\"bar\", x='city', y='avgRating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbJdSKC84jnP"
   },
   "source": [
    "# Exercise 2: The case of London (30 pts)\n",
    "\n",
    "In this exercise you must use Spark to do the data processing. \n",
    "* For parts where you present tabular data, this entails calling `toPandas` as the final step of your query. \n",
    "* For parts requiring visualisation, the `toPandas` call should be followed only by functions necessary to customize the plotting/layout steps (i.e. no data processing take place after your spark dataframe is materialized). \n",
    "* You may need multiple queries to solve the individual parts.\n",
    "\n",
    "Your dataframe is a subset of `df_listings_analysis` and should be named `df_listings_london`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:36:05.783859Z",
     "start_time": "2021-10-31T14:36:05.781830Z"
    },
    "id": "aeVpGvZF4jnP"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "df_listings_london = df_listings_analysis.filter(df_listings_analysis.city == 'London').cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-KJqz1H4jnP"
   },
   "source": [
    "## Part 1: Price distribution (5 pts)\n",
    "\n",
    "Compute and visualise the distribution of prices, for all prices up to and including the 95-percentile. Additionally, compute and visualise the distribution of prices, for all prices above the 95-percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:36:15.616156Z",
     "start_time": "2021-10-31T14:36:15.613196Z"
    },
    "id": "kWc-mqph4jnP"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(13,5))\n",
    "_ = df_listings_london.filter(df_listings_analysis.price <= df_listings_london.approxQuantile('price',[0.95],0)[0]).toPandas()\\\n",
    "        .plot.hist(ax=axes[0],bins=50)\n",
    "axes[0].title.set_text('price <= quantile(0.95)')\n",
    "_ = df_listings_london.filter(df_listings_analysis.price > df_listings_london.approxQuantile('price',[0.95],0)[0]).toPandas()\\\n",
    "        .plot.hist(ax=axes[1],bins=50)\n",
    "axes[1].title.set_text('price > quantile(0.95)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2Oa37Ck4jnP"
   },
   "source": [
    "## Part 2: Prices by type of property (5 pts)\n",
    "\n",
    "Compute and visualise the average price and average rating per type of property, for property types with 75 or more listings. \n",
    "\n",
    "Your visualisation should be a single bar chart with two y-axes and two bars per property type. The x-axis should be ordered by average rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:36:21.187516Z",
     "start_time": "2021-10-31T14:36:21.179099Z"
    },
    "id": "b5jx-W9O4jnP"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "pandas_temp = df_listings_london.groupBy(\"property_type\")\\\n",
    "    .agg(F.count('property_type').alias('listings'), F.avg('review_scores_rating').alias('avgRating'), F.avg(\"price\").alias(\"avgPrice\")) \\\n",
    "    .orderBy('listings', ascending=False).limit(75).orderBy(\"avgRating\",ascending=False ).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "_= pandas_temp.plot(ax= ax , kind=\"bar\", x='property_type', y='avgRating')\n",
    "ax2 = ax.twinx()\n",
    "_ = pandas_temp.plot(ax= ax2 , kind=\"bar\", x='property_type', color=\"red\", alpha=.5, y='avgPrice')\n",
    "ax2.title.set_text('average price and average rating, left y-xis = avgRating, right y-axis = avgPrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvUyAWA94jnP"
   },
   "source": [
    "## Part 3: Best offering in the neighbourhood (10 pts)\n",
    "\n",
    "The value of a listing is its rating divided by its price. Compute and display a dataframe (with the columns you selected in Exercise 1 and those computed in this part) with the 3 highest valued listings in each neighbourhood, and having a value above 5. Make sure to use the `neighbourhood_cleansed` column in your computations.\n",
    "\n",
    "Computing ranks based on value can be achieved using `pyspark.sql.window.Window`. This may produce equal ranks (i.e. when the value of two listings are the same).\n",
    "\n",
    "Remember to use `pd.set_option('display.max_rows', <n>)` with appropriate `<n>` so all rows are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T15:30:05.393213Z",
     "start_time": "2021-10-27T15:30:03.259525Z"
    },
    "id": "8lmAGdHw4jnQ"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "df_listings_london= df_listings_london.withColumn('value',df_listings_london.review_scores_rating/df_listings_london.price)\n",
    "window = Window.partitionBy(\"neighbourhood_cleansed\").orderBy(col('value').desc())\n",
    "df_listings_london.withColumn(\"rank\", rank().over(window)).filter(col('rank') <= 3).filter(df_listings_london.value > 5)\\\n",
    ".toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZdZ5-FE4jnQ"
   },
   "source": [
    "## Part 4: Activity by month (5 pts)\n",
    "\n",
    "Activity is given by the number of reviews received in a given time period. Compute and visualise the activity based on month, that is, the total number of reviews given in January, February, etc..\n",
    "\n",
    "Your additional data source is [this zip archive](https://data-download.compute.dtu.dk/c02807/reviews_london.csv.zip) which you must uncompress and place in the same folder as this notebook. It is loaded in the next cell and named `df_reviews_london`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T15:30:07.720386Z",
     "start_time": "2021-10-27T15:30:05.404386Z"
    },
    "id": "MGtA6uzq4jnQ"
   },
   "outputs": [],
   "source": [
    "df_reviews_london = load_csv_as_dataframe('reviews_london.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:36:42.395920Z",
     "start_time": "2021-10-31T14:36:42.393979Z"
    },
    "id": "NohJUbEg4jnQ"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month\n",
    "# your code goes here\n",
    "_ = df_reviews_london.withColumn('month',month(df_reviews_london.date))\\\n",
    "        .groupby('month').agg(F.count('month').alias('activity')).orderBy('month')\\\n",
    "        .toPandas().plot(kind=\"bar\", x='month', y='activity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "So8nbHnR4jnQ"
   },
   "source": [
    "## Part 5: Reviews per listing (5 pts)\n",
    "\n",
    "Each London listing has received 0 or more reviews. \n",
    "\n",
    "Display a dataframe showing 1) The number of listings, 2) The average number of reviews a listing receives, 3) The standard deviation of the reviews per listing distribution, 4) The minimum number of reviews any listing has received, and 5) The maximum number of reviews any listing has received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:36:48.293254Z",
     "start_time": "2021-10-31T14:36:48.291307Z"
    },
    "id": "xnH01VPh4jnQ"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "window = Window.partitionBy('listing_id')\n",
    "df_reviews_london.withColumn('numRevPrList', F.count('listing_id').over(window))\\\n",
    ".select(F.countDistinct('listing_id').alias('NumberOfListings'), \n",
    "        F.avg('numRevPrList').alias('avgNumRevPrListing'), \n",
    "        F.stddev('numRevPrList').alias('std'),\n",
    "        F.min('numRevPrList').alias('minReviews'),\n",
    "        F.max('numRevPrList').alias('maxReviews'))\\\n",
    ".limit(10).toPandas().T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x62poLc34jnQ"
   },
   "source": [
    "# Exercise 3: Word sentiment (45 pts)\n",
    "\n",
    "In this exercise you must use Spark to do the data processing. For parts where you present tabular data, this entails calling `toPandas` as the final step of your query. You may need multiple queries to solve the individual parts.\n",
    "\n",
    "The goal here is to determine what sentiment (positive or negative) words in reviews have. Roughly speaking, we want each word to be assigned a score based on the rating of the reviews in which the word occurs in the review comment. We'd expect words such as \"clean\", \"comfortable\", \"superhost\" to receive high scores, while words such as \"unpleasant\", \"dirty\", \"disgusting\" would receive low scores.\n",
    "\n",
    "As individual reviews do not have a rating, we'll consider the rating of individual reviews to be the rating of its related listing (i.e. assuming each review gave the average rating (`review_scores_rating`) of the listing). \n",
    "\n",
    "The score of a word is given by the mean review rating over the reviews in which that word occurs in the comment. We require words to appear in at least 0.5% (1 in 200) listings, and to be at least 4 characters, for it to have a defined score.\n",
    "\n",
    "Formally, when a word $w$ occurs in at least $0.5\\%$ of listings and $|w| > 3$, its score is\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "score(w) = \\frac{1}{|C_w|}\\sum_{comment \\in C_w} \\text{review_rating}(comment)\n",
    "\\end{align*}\n",
    "$\n",
    ", where \n",
    "* $C_w = \\{comment \\mid w \\text{ occurs in } \\text{clean_text}(comment)\\}$, the set (so no duplicates) of comments in which $w$ occurs, and\n",
    "* $\\text{clean_text}(comment)$ is the result of your `clean_text` function defined below, and\n",
    "* $\\text{review_rating}(comment)$ is the `review_scores_rating` of the listing which this $comment$ is related to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFNxM4PS4jnQ"
   },
   "source": [
    "## Part 1: Toy data (15 pts)\n",
    "\n",
    "To get started we'll consider a toy example where the input is `df_sentiment_listings_toy` and `df_sentiment_reviews_toy` defined in the next code cell. You should provide an implementation of `calculate_word_scores_toy` in the subsequent code cell. Your implementation should result in a query that when given the toy example dataframes as input and is materialized with `toPandas()` produces this table:\n",
    "\n",
    "|    | word   |   word_score |   listing_occurences |   word_occurences |   comment_occurences |\n",
    "|---:|:-------|-------------:|---------------------:|------------------:|---------------------:|\n",
    "|  0 | aaaa   |      7       |                    3 |                 5 |                    5 |\n",
    "|  1 | bbbb   |      6.66667 |                    2 |                 3 |                    3 |\n",
    "|  2 | eeee   |      0       |                    1 |                 1 |                    1 |\n",
    "|  3 | dddd   |      5       |                    1 |                 1 |                    1 |\n",
    "|  4 | cccc   |      5       |                    2 |                 2 |                    2 |'\n",
    "\n",
    "Observe that `word_occurences` and `comment_occurences` are the same as words occuring multiple times in a comment are counted once, and that `clean_text` is used to ignore casing and discard non-words. Additionally, any word occuring at least once will occur in more than 1 out of 200 listings on this toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T15:30:20.311738Z",
     "start_time": "2021-10-27T15:30:20.131208Z"
    },
    "id": "p2_01YSQ4jnQ"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema_listings = StructType([\n",
    "    StructField('id', StringType(), True),\n",
    "    StructField('review_scores_rating', StringType(), True),\n",
    "])\n",
    "data_listings = [\n",
    "    {'id': '0', 'review_scores_rating': '10'},\n",
    "    {'id': '1', 'review_scores_rating': '5'},\n",
    "    {'id': '2', 'review_scores_rating': '0'},\n",
    "]\n",
    "df_sentiment_listings_toy = spark.createDataFrame(data_listings, schema_listings)\n",
    "\n",
    "schema_reviews = StructType([\n",
    "    StructField('listing_id', StringType(), True),\n",
    "    StructField('id', StringType(), True),\n",
    "    StructField('comments', StringType(), True),\n",
    "])\n",
    "data_reviews = [\n",
    "    {'listing_id': '0', 'id': '100', 'comments': 'aaaa bbbb          cccc'},\n",
    "    {'listing_id': '0', 'id': '101', 'comments': 'aaaa bbbb '},\n",
    "    {'listing_id': '0', 'id': '102', 'comments': 'aaaa aAAa          aaaa'},\n",
    "    {'listing_id': '1', 'id': '103', 'comments': 'Aaaa bbb ccc'},\n",
    "    {'listing_id': '1', 'id': '104', 'comments': 'dddd %ˆ&*'},\n",
    "    {'listing_id': '2', 'id': '105', 'comments': 'AaaA'},\n",
    "    {'listing_id': '2', 'id': '106', 'comments': 'bbbb ccc e&eˆˆee'},\n",
    "    {'listing_id': '2', 'id': '107', 'comments': 'cccc cccc'},\n",
    "]\n",
    "\n",
    "df_sentiment_reviews_toy = \\\n",
    "    spark.createDataFrame(data_reviews, schema_reviews) \\\n",
    "        .select(F.col('listing_id'), F.col('id').alias('comment_id'), F.col('comments'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment_listings_toy.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment_reviews_toy.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T15:30:24.249616Z",
     "start_time": "2021-10-27T15:30:20.312564Z"
    },
    "id": "ybRkPjCD4jnR"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import lower, regexp_replace, array_distinct, explode    \n",
    "\n",
    "def clean_text(col):\n",
    "    \"\"\"\n",
    "        Cleans the text (comment) associated with col. The\n",
    "        cleaning should:\n",
    "            1) Lower case the text\n",
    "            2) Turn multiple whitespaces into single whitespaces\n",
    "            3) Remove anything but letters, digits and whitespaces\n",
    "        \n",
    "        :col: A Spark Column object containing text data\n",
    "        :returns: A Spark Column object.\n",
    "    \"\"\"\n",
    "    \n",
    "    col = re.sub(\"[^A-Za-z0-9W\\s]\", \"\", re.sub(\"\\s\\s+\",\" \", str(col))).strip().lower()\n",
    "    \n",
    "    return col\n",
    "\n",
    "    \n",
    "clean_text_udf = F.udf(clean_text)\n",
    "\n",
    "df_sentiment_reviews_toy = df_sentiment_reviews_toy.withColumn(\"clean_comments\", clean_text_udf(F.col(\"comments\")))  \n",
    "\n",
    "\n",
    "def calculate_word_scores_toy(df_list, df_rev):\n",
    "    \"\"\"\n",
    "        Calculates the word score over listings in df_list and\n",
    "        reviews in df_rev. The table produced should have the \n",
    "        same columns as specified in part 1.\n",
    "        \n",
    "        :returns: A pandas DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    df_joined = df_sentiment_reviews_toy.join(df_sentiment_listings_toy, \n",
    "                  df_sentiment_listings_toy['id'] == df_sentiment_reviews_toy['listing_id']).drop('id')\n",
    "    \n",
    "    df_joined = df_joined.withColumn(\"word\", F.explode(array_distinct(F.split(\"clean_comments\",\" \")))) \\\n",
    "    .filter(length(\"word\")>3).groupBy(\"word\")\\\n",
    "    .agg(F.mean(\"review_scores_rating\").alias(\"word_score\"),\\\n",
    "        countDistinct(\"listing_id\").alias(\"listing_occurences\"), \\\n",
    "        countDistinct(\"comment_id\").alias(\"word_occurences\"), \\\n",
    "        countDistinct(\"comment_id\").alias(\"comment_occurences\"))\n",
    "\n",
    "\n",
    "    return df_joined\n",
    "    \n",
    "\n",
    "df = calculate_word_scores_toy(df_sentiment_listings_toy, df_sentiment_reviews_toy)\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wF0-Et6Y4jnR"
   },
   "source": [
    "## Part 2: London comments (15 pts)\n",
    "\n",
    "In this part we'll calculate word scores for the comments related to London listings only. You should implement `count_relevant_listings` and `calculate_word_scores` (it will be an extension of your function from part 1) below. See the mathematical definition and docstrings for intended behaviour.\n",
    "\n",
    "The function `calculate_word_scores` should return the top 10 and bottom 10 words by score. You should **not** use caching in your function.\n",
    "\n",
    "Make sure your satisfy all conditions for a word to be scored (e.g. correctly calculating how many total listings scores are computed over). You should also consider whether your query is optimally structured in terms of computation time. Moreover, `pd.set_option('display.max_rows', <n>)` should be set with sufficiently high `n` to show all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T13:35:09.413747Z",
     "start_time": "2021-10-31T13:35:09.386493Z"
    },
    "id": "NL05dnDH4jnR"
   },
   "outputs": [],
   "source": [
    "# your code goes here   \n",
    "\n",
    "\n",
    "def count_relevant_listings(df_list, df_rev):\n",
    "    \"\"\"\n",
    "        Calculates the number of listings in df_list that has a \n",
    "        review in df_rev. A listing that is reviewed more than once\n",
    "        should only count as one.\n",
    "        \n",
    "        :returns: An integer \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    df_joined = df_reviews_london.join(df_listings_london, \n",
    "                    df_listings_london['id'] == df_reviews_london['listing_id']).drop('id')\n",
    "    \n",
    "    \n",
    "    listings = df_joined.select(F.countDistinct('listing_id')).first()[0]\n",
    "    \n",
    "    return listings\n",
    "\n",
    "    \n",
    "\n",
    "def calculate_word_scores(df_list, df_rev, listings_count):\n",
    "    \"\"\"\n",
    "        Calculates the word score over listings in df_list and\n",
    "        reviews in df_rev. The value of listings_count should \n",
    "        be used to filter out words not occuring frequently enough\n",
    "        in comments. The table produced should have the same columns\n",
    "        as in part 1 of this exercise.\n",
    "        \n",
    "        :returns: A pandas DataFrame containing the top 10 and \n",
    "        bottom 10 words based on their word score, sorted by word_score.\n",
    "    \"\"\"\n",
    "        \n",
    "    df_rev = df_rev.withColumn(\"clean_comments\", clean_text_udf(F.col(\"comments\"))) \\\n",
    "                        .withColumnRenamed('id', 'comment_id')\n",
    "    \n",
    "    df_joined = df_rev.join(df_list, \n",
    "            df_list['id'] == df_rev['listing_id']).drop('id')\n",
    "    \n",
    "    df_words = df_joined.withColumn(\"word\", F.explode(array_distinct(F.split(\"clean_comments\",\" \")))) \\\n",
    "                        .filter(length(\"word\")>3).groupBy(\"word\")\\\n",
    "                        .agg(F.mean(\"review_scores_rating\").alias(\"word_score\"),\\\n",
    "                            countDistinct(\"listing_id\").alias(\"listing_occurences\"), \\\n",
    "                            countDistinct(\"comment_id\").alias(\"word_occurences\"), \\\n",
    "                            countDistinct(\"comment_id\").alias(\"comment_occurences\")) \\\n",
    "                        .filter(F.col('listing_occurences') >= 0.5 / 100 * listings_count) \\\n",
    "                        .orderBy(col(\"word_score\").desc())\n",
    "\n",
    "    return pd.concat([df_words.toPandas().head(10), df_words.toPandas().tail(10)])\n",
    "\n",
    "\n",
    "\n",
    "relevant_listings_count_london = count_relevant_listings(df_listings_london, df_reviews_london)\n",
    "\n",
    "calculate_word_scores(df_listings_london, df_reviews_london, relevant_listings_count_london)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T13:37:21.918401Z",
     "start_time": "2021-10-31T13:35:10.034363Z"
    },
    "id": "ZoUPZ6F-4jnR"
   },
   "outputs": [],
   "source": [
    "# should not be modified\n",
    "from IPython.display import display\n",
    "\n",
    "relevant_listings_count_london = count_relevant_listings(df_listings_london, df_reviews_london)\n",
    "word_scores_london_timing = %timeit -o -n1 -r1 display( \\\n",
    "    calculate_word_scores(df_listings_london, \\\n",
    "                          df_reviews_london, \\\n",
    "                          relevant_listings_count_london) \\\n",
    ")\n",
    "\n",
    "word_scores_london_timing.best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAPvcNbU4jnR"
   },
   "source": [
    "## Part 3: Scalability (10 pts)\n",
    "\n",
    "The listings from London make up a little less than 2% of the entire set of listings. In this part we're interested in how the amount of input data impacts computation time, that is, how `calculate_word_scores` scales as data increases. To this end, we've made multiple samples of the dataset of varying sizes.\n",
    "\n",
    "The experiment reuses `count_relevant_listings` and `calculate_word_scores` that you implemented in part 2. Code needed for this part is provided to you. \n",
    "\n",
    "Your task is to obtain the data sources, run the code cells below, and explain the results you get. Specifically, you must explain any non-linear relationship between data size and computation time, using the markdown cell at the end of this part. In finding explanations, using the Spark UI to investigate the anatomy of your queries may prove valuable. Once you've found an explanation, state a potential solution to remedy the issue. Lastly, include a paragraph stating the specifications of your computer hardware (memory, CPU cores and clock speed, solid state disk or not) on which the experiment has been run.\n",
    "\n",
    "*Implementation note* Make sure you've properly configured `spark.driver.memory` (it requires a kernel restart to update the value). It may be that your query fails on the larger samples due to running out of compute resources. This is likely caused by a suboptimal `calculate_word_scores`, but can be from reaching the limits of your hardware. If you think the latter is the case, argue for this perspective in the markdown cell.\n",
    "\n",
    "Your data sources are (uncompress and place in the same directory as this notebook):\n",
    "* 0.25%: [listings](https://data-download.compute.dtu.dk/c02807/listings_0-dot-25percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_0-dot-25percent.csv.zip)\n",
    "* 0.5%: [listings](https://data-download.compute.dtu.dk/c02807/listings_0-dot-5percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_0-dot-5percent.csv.zip)\n",
    "* 1%: [listings](https://data-download.compute.dtu.dk/c02807/listings_1-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_1-dot-0percent.csv.zip)\n",
    "* 2%: [listings](https://data-download.compute.dtu.dk/c02807/listings_2-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_2-dot-0percent.csv.zip)\n",
    "* 4%: [listings](https://data-download.compute.dtu.dk/c02807/listings_4-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_4-dot-0percent.csv.zip)\n",
    "* 8%: [listings](https://data-download.compute.dtu.dk/c02807/listings_8-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_8-dot-0percent.csv.zip)\n",
    "* 12.5%: [listings](https://data-download.compute.dtu.dk/c02807/listings_12-dot-5percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_12-dot-5percent.csv.zip)\n",
    "* 16%: [listings](https://data-download.compute.dtu.dk/c02807/listings_16-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_16-dot-0percent.csv.zip)\n",
    "* 25%: [listings](https://data-download.compute.dtu.dk/c02807/listings_25-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_25-dot-0percent.csv.zip)\n",
    "* 50%: [listings](https://data-download.compute.dtu.dk/c02807/listings_50-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_50-dot-0percent.csv.zip)\n",
    "* 75%: [listings](https://data-download.compute.dtu.dk/c02807/listings_75-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_75-dot-0percent.csv.zip)\n",
    "* 100%: [listings](https://data-download.compute.dtu.dk/c02807/listings_100-dot-0percent.csv.zip), [reviews](https://data-download.compute.dtu.dk/c02807/reviews_100-dot-0percent.csv.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T23:46:48.667540Z",
     "start_time": "2021-10-28T23:46:48.663425Z"
    },
    "id": "u9BPHTsQ4jnR"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def calculate_word_scores_timed(percent_str):\n",
    "    \"\"\"\n",
    "        Calculates word scores over a sampled dataset indicated\n",
    "        by percent_str.\n",
    "        \n",
    "        :returns: A dictionary with benchmarking information and\n",
    "        the calculated values.\n",
    "    \"\"\"\n",
    "    df_listings = load_csv_as_dataframe(f'listings_{percent_str}percent.csv')\n",
    "    df_reviews = load_csv_as_dataframe(f'reviews_{percent_str}percent.csv')\n",
    "    \n",
    "    listings_count = count_relevant_listings(df_listings, df_reviews)\n",
    "\n",
    "    start = time.time()\n",
    "    df_word_scores = calculate_word_scores(df_listings, df_reviews, listings_count)\n",
    "    end = time.time()\n",
    "    return {\n",
    "        'percentage': float(percent_str.replace('-dot-', '.')), \n",
    "        'time_spent': f\"{end - start:.2f}\", \n",
    "        'relevant_listings': listings_count, \n",
    "        'df': df_word_scores\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T00:35:02.491711Z",
     "start_time": "2021-10-28T23:46:49.516730Z"
    },
    "id": "0eutRq674jnR"
   },
   "outputs": [],
   "source": [
    "data_percentages = [\n",
    "    '0-dot-25', '0-dot-5', '1-dot-0', '2-dot-0', '4-dot-0', '8-dot-0',\n",
    "    '12-dot-5', '16-dot-0', '25-dot-0'\n",
    "]\n",
    "score_data = {\n",
    "    percentage_str: calculate_word_scores_timed(percentage_str) for percentage_str in data_percentages\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T00:35:02.502845Z",
     "start_time": "2021-10-29T00:35:02.493477Z"
    },
    "id": "6gn8hhmq4jnR"
   },
   "outputs": [],
   "source": [
    "score_data['50-dot-0'] = calculate_word_scores_timed('50-dot-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T00:35:02.506905Z",
     "start_time": "2021-10-29T00:35:02.503911Z"
    },
    "id": "o6S0UVl74jnS"
   },
   "outputs": [],
   "source": [
    "score_data['75-dot-0'] = calculate_word_scores_timed('75-dot-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T00:35:02.511277Z",
     "start_time": "2021-10-29T00:35:02.510002Z"
    },
    "id": "Co5DP5jH4jnS"
   },
   "outputs": [],
   "source": [
    "score_data['100-dot-0'] = calculate_word_scores_timed('100-dot-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T00:35:02.580856Z",
     "start_time": "2021-10-29T00:35:02.512026Z"
    },
    "id": "KXicfeA-4jnS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_scores_scaling = pd.DataFrame(score_data).T.convert_dtypes()\n",
    "df_scores_scaling.time_spent = df_scores_scaling.time_spent.astype(float)\n",
    "\n",
    "# Access to word scores of 2 percent data: df_scores_scaling.loc['2-dot-0'].df\n",
    "df_scores_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T00:35:02.872708Z",
     "start_time": "2021-10-29T00:35:02.582105Z"
    },
    "id": "pEviNX0y4jnS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "\n",
    "lower_range = ['0-dot-25', '0-dot-5', '1-dot-0', '2-dot-0', '4-dot-0', '8-dot-0', '16-dot-0']\n",
    "df_scores_scaling[df_scores_scaling.index.isin(lower_range)] \\\n",
    "    .plot.line(x='percentage', y='time_spent', ax=axes[0], style='-o', title='Lower range')\n",
    "df_scores_scaling[~df_scores_scaling.index.isin(lower_range)] \\\n",
    "    .plot.line(x='percentage', y='time_spent', ax=axes[1], style='-o', title='Upper range')\n",
    "_ = df_scores_scaling \\\n",
    "    .plot.line(x='percentage', y='time_spent', ax=axes[2], style='-o', title='All')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yf5c4daR4jnS"
   },
   "source": [
    "*Your explanation to the questions outlined at the start of this part goes here. Make sure you've addressed all questions asked.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-61VA-34jnS"
   },
   "source": [
    "## Part 4: Robustness (5 pts)\n",
    "\n",
    "In this part we'll explore robustness of our word scores, using the values we computed in part 3. We'll do so by comparing top/bottom words for three different samples of the dataset. Specifically, the scores from your maximum (e.g. 100%) computed sample are to be compared with the 12.5% and 2.0% scores.\n",
    "\n",
    "Compute and display a dataframe that accounts for any word found in either of the three samples' top/bottom words, and additionally shows the related `word_score` and `word_occurences` values.\n",
    "\n",
    "Note that `df_scores_scaling.loc['100-dot-0'].df` provides the word scores dataframe of the 100% sample (similarly for the other two). For this part you should rely on pandas functionality only.  Moreover, `pd.set_option('display.max_rows', <n>)` should be set with sufficiently high `n` to show all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:37:40.614597Z",
     "start_time": "2021-10-31T14:37:40.612561Z"
    },
    "id": "9jrmvVmM4jnS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQyEFla44jnS"
   },
   "source": [
    "# Exercise 4: Transactions analysis (55 pts)\n",
    "\n",
    "In this exercise the goal is to analyse historical business transactions (sales of parts to other companies), and derive insights about both products and customers.\n",
    "\n",
    "The company X produces and globally sells gadget parts to a number of other companies. You requested the sales department of X to provide you with access to the customer and sales transactions database. To your horror, you've found no such database exists, but the data is instead manually maintained in a spreadsheet (error-prone solution). Intrepid as you are, you've accepted to receive the spreadsheet data as a `.csv`, realizing already data cleaning will be necessary.\n",
    "\n",
    "Your first step (parts 1 and 2) is to clean the data after which you will derive insights about X's business operations (parts 3 and 4).\n",
    "\n",
    "The input data is available here: [transactions.csv](http://courses.compute.dtu.dk/02807/2021/projects/project2/transactions.csv)\n",
    "\n",
    "**Using SQL**\n",
    "\n",
    "In this final exercise you must write SQL to do the data processing in parts 3 and 4. This entails using `psql.sqldf` to execute your queries (up against `df_transactions_cleaned`) which will return a pandas dataframe. Each question should be answered with a *single* query. For visualisation the `psql.sqldf` call should be followed only by functions necessary to customize the plotting/layout steps or reshape the dataframe (i.e. no data processing take place after your SQL statement is materialized as a pandas dataframe).\n",
    "\n",
    "In part 1 and 2 of this exercise, you should make use of pandas functionality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SM2KEdqu4jnS"
   },
   "source": [
    "## Part 1: Data cleaning (15 pts)\n",
    "\n",
    "For each column in the dataframe, investigate and **correct** problematic aspects such as,\n",
    "* Missing values: Insert meaningful values (data imputation). Detectable as `np.nan`'s. A typical value for imputation is the *mode* (most frequent value) of the distribution. If no proper data imputation is possible, you may resort to dropping rows.\n",
    "* Incorrect values: Typos and other data mishaps are present as values are manually entered. Detectable as low-prevalence categorical values, or ambigious data links (e.g. company listed in multiple countries). If no proper value correction is possible, you may resort to dropping rows.\n",
    "\n",
    "In both cases, your strategy for replacing values should be data-driven, that is, shaped by the patterns you observe in the data. It is allowed to skip correcting the data (and instead drop the rows) if few rows are improved by your corrections. If in doubt, do the correction.\n",
    "\n",
    "After all your cleaning steps are completed, you should run the `PandasProfiler` on your cleaned dataset, which should now contain 0% missing cells. Lastly, summarize the issues you identified and how you addressed them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9EAimzb4jnS"
   },
   "source": [
    "### Read, profile and explain\n",
    "\n",
    "As the first step, load the data naming the dataframe `df_transactions`, and make a copy named `df_transactions_cleaned` on which your data cleaning steps will be done. Establish an overview using `PandasProfiler` (but realize there's more to cleaning than what this tool will let you know). Write a paragraph on what the data is about (e.g. what does a row constitute), and a paragraph on what the profile report tells you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:37:53.403596Z",
     "start_time": "2021-10-31T14:37:53.400458Z"
    },
    "id": "A4gokJNI4jnT"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "#!pip install pandasql==0.7.3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandasql as psql\n",
    "import math\n",
    "from pandas_profiling import ProfileReport \n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "\n",
    "df_transations = pd.read_csv(\"transactions.csv\")\n",
    "df_transations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(df_transations, title=\"Pandas Profiling Report\")\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transations.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KwNFkhZ4jnT"
   },
   "source": [
    "*From above it can be noticed which columns contain NaN values need to be replaced or removed. In addition, some of the columns, for instance city, contain special characters we will try to remove the special characters and keep only the name of the city-country-company. For the price we will try to split the actual price from the currency sign in two different columns. Then we will first create and populate the `prices_euro` column and the NaN will be populated with the median to avoid outliers, once the column does not contain NaN values we will fill the NaN in the price column. We follow this approach because the exchange ratio between the currencies differs a lot. Furthermore, the prices contain negative values which we keep them since a transaction  can be positive or negative if for example is sale or return order respectively. In some cases for almost all the NaN contained columns there were values like \"-\", \"void\" or spelling mistakes these we identify manually and replace them since there were not a lot. If the price was NaN we populate the currency by USD. The columns with no NaN values we used to group by them and populate the columns with NaN like country, city, part. From the profiler above we can see we have 1.8% missing data, and also some of the column are high correlated. The column date we tried to convert to date type in order to use them below in the SQL queries easily.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typos-regex-city-country-company\n",
    "def keep_only_name(c_name):\n",
    "    if c_name == np.nan:\n",
    "        return np.nan\n",
    "    # avoid special characters in name city-country\n",
    "    return re.sub(r\"[^a-zA-Z0-9]+\", '', str(c_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uCcxzh54jnT"
   },
   "source": [
    "### Country column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:38:10.817218Z",
     "start_time": "2021-10-31T14:38:10.814361Z"
    },
    "id": "UQobGiSQ4jnT"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "df_transations['country'] = df_transations.groupby(\"company\")['country'].apply(lambda x: x.fillna(x.mode()[0]))\n",
    "df_transations[\"country\"] = df_transations[\"country\"].apply(lambda c: keep_only_name(c))\n",
    "\n",
    "mask_portuga = df_transations[\"country\"] == \"Portuga\"\n",
    "index_portuga = df_transations.index[mask_portuga]\n",
    "df_transations = df_transations.drop(index_portuga)\n",
    "\n",
    "mask_us = df_transations[\"country\"] == \"US\"\n",
    "index_us = df_transations.index[mask_us]\n",
    "df_transations = df_transations.drop(index_us)\n",
    "print(\"Unique Countries: \", df_transations[\"country\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transations[df_transations['company'] == 'Flipstorm'].country.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For the above company we found that exists in two countries in Greece with 861 rows and in France with 332 so we will replace the country value to all the rows with Greece*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transations.loc[df_transations['company'] == 'Flipstorm', 'country']= 'Greece'\n",
    "df_transations.loc[df_transations['company'] == 'Flipstorm', 'city']= 'Athens'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnxk7N984jnT"
   },
   "source": [
    "### Company column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:38:23.615156Z",
     "start_time": "2021-10-31T14:38:23.612981Z"
    },
    "id": "Z1vr1z1V4jnT"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "df_transations[\"company\"] = df_transations[\"company\"].apply(lambda c: keep_only_name(c))\n",
    "\n",
    "\n",
    "mask_lajo = df_transations[\"company\"] == \"Laj0\"\n",
    "index_lajo = df_transations.index[mask_lajo]\n",
    "df_transations = df_transations.drop(index_lajo)\n",
    "\n",
    "mask = (df_transations[\"company\"] == 'aa') | (df_transations[\"company\"] == 'a')\\\n",
    "        | (df_transations[\"company\"] == '')\n",
    "index_company = df_transations.index\n",
    "index_to_drop_company = list(index_company[mask])\n",
    "df_transations = df_transations.drop(index_to_drop_company)\n",
    "\n",
    "mask_th = df_transations[\"company\"] == \"Thoughtmixz\"\n",
    "index_th = df_transations.index[mask_th]\n",
    "df_transations = df_transations.drop(index_th)\n",
    "\n",
    "\n",
    "mask_nt = df_transations[\"company\"] == \"Ntagz\"\n",
    "index_nt = df_transations.index[mask_nt]\n",
    "df_transations = df_transations.drop(index_nt)\n",
    "\n",
    "print(\"Unique Countries: \", df_transations[\"company\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzkzURFj4jnT"
   },
   "source": [
    "### City column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T18:56:56.402705Z",
     "start_time": "2021-10-27T18:56:56.401612Z"
    },
    "id": "zOQjBe-94jnT"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "df_transations['city'] = df_transations['city'].apply(lambda c: keep_only_name(c))\n",
    "mask_duss = df_transations[\"city\"] == \"Dsseldorf\"\n",
    "index_duss = df_transations.index[mask_duss]\n",
    "df_transations.loc[index_duss, \"city\"] = \"Dusseldorf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SN671FEX4jnT"
   },
   "source": [
    "### Parts column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T18:56:56.644628Z",
     "start_time": "2021-10-27T18:56:56.643615Z"
    },
    "id": "nMHzx5rl4jnT"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "df_transations['part'] = df_transations.groupby(\"company\")['part'].apply(lambda x: x.fillna(x.mode()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Stk-AVkM4jnT"
   },
   "source": [
    "### Price column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# price-currency\n",
    "def split_prices(price):\n",
    "    if str(price) == 'nan' :\n",
    "        return np.nan, 'USD'\n",
    "    curr = \"$\"\n",
    "    curr_dict = {'€': 'EUR', '£': \"GBP\", '$': \"USD\", '¥': \"JPY\"}\n",
    "    num = re.findall(\"-?[0-9]+[\\.]*[0-9]*\", price)\n",
    "    curr_list = re.split(\"-?[0-9]+[\\.]*[0-9]*\", price)\n",
    "    \n",
    "    if not num:\n",
    "        num = np.nan\n",
    "    else:\n",
    "        num = num[0]\n",
    "    \n",
    "    if curr_list:\n",
    "        for item in curr_list:\n",
    "            if item != \"\":\n",
    "                curr = item\n",
    "    else:\n",
    "        curr = \"$\"\n",
    "        \n",
    "    if curr_dict.get(curr) is None:\n",
    "        curr = \"$\"\n",
    "    \n",
    "    \n",
    "    return float(num), curr_dict.get(curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T18:56:56.662243Z",
     "start_time": "2021-10-27T18:56:56.660643Z"
    },
    "id": "bGb4G97k4jnU"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "index_price = df_transations.index\n",
    "mask1  = df_transations[\"price\"] == \"-\"\n",
    "mask2 = df_transations[\"price\"] == \"void\"\n",
    "\n",
    "mask1_index = list(index_price[mask1])\n",
    "mask2_index = list(index_price[mask2])\n",
    "\n",
    "df_transations.price.iloc[mask1_index+mask2_index] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_transations['price'].apply(lambda p: split_prices(p))\n",
    "df_temp = pd.DataFrame(df_temp.tolist(), index=df_temp.index)\n",
    "df_temp.columns = [\"price\", \"curr\"]\n",
    "# df_temp.price = df_temp.groupby(['curr'], sort=False)['price'].fillna(df_temp['price'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transations[\"price\"], df_transations[\"currency_code\"] = df_temp[\"price\"], df_temp[\"curr\"]\n",
    "df_transations.currency_code.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9TgwrNZ4jnU"
   },
   "source": [
    "### Date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:38:43.270635Z",
     "start_time": "2021-10-31T14:38:43.268585Z"
    },
    "id": "am9CNN594jnU"
   },
   "outputs": [],
   "source": [
    "#your code goes here\n",
    "# we used coerce to avoid date out of range by reaplacing them with nan because there are only two values\n",
    "df_transations['date'] =  pd.to_datetime(df_transations['date'], errors='coerce')\n",
    "df_transations['date'] = df_transations.groupby(['company'], sort=False)['date'].fillna(df_transations['date'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVEOeN4C4jnU"
   },
   "source": [
    "## Part 2: Standardise prices (5 pts)\n",
    "\n",
    "Transaction prices are recorded in the local currency of the client (EUR, GBP, USD or JPY). You will need to convert these prices from local currency into the common currency (chosen here as) EUR, for comparability. These standardised prices should be added as a column to the dataframe called `prices_euro`.\n",
    "\n",
    "Consider a two step process where you 1) Identify what currency has been used, and 2) Calculate the price conversion. Step 1 may reveal the data is still not completely clean (so either correct by impute or drop). For Step 2 look up exchange rates on the Internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T18:56:58.558049Z",
     "start_time": "2021-10-27T18:56:58.556556Z"
    },
    "id": "VTFTFPMX4jnU"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "df_transations[\"price\"], df_transations[\"currency_code\"] = df_temp[\"price\"], df_temp[\"curr\"]\n",
    "df_transations.currency_code.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exchange(cc):\n",
    "    ratio = {\"EUR\":1, \"GBP\":1.19, \"USD\":0.89, \"JPY\":0.007}\n",
    "    \n",
    "    return ratio.get(cc)\n",
    "    \n",
    "    \n",
    "df_transations['ex_ratio'] = df_transations[\"currency_code\"].apply(lambda cc: exchange(cc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transations[\"prices_euro\"] = df_transations[\"price\"] * df_transations[\"ex_ratio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transations.prices_euro = df_transations.groupby(['currency_code'], sort=False)['prices_euro'].fillna(df_transations['prices_euro'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:38:52.078805Z",
     "start_time": "2021-10-31T14:38:52.076381Z"
    },
    "id": "vOuZ6QGY4jnU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "# exchange euro prices populated with the median to the original currency\n",
    "df_transations[\"price\"] = df_transations[\"prices_euro\"] / df_transations[\"ex_ratio\"]\n",
    "df_transations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTnK1fMF4jnU"
   },
   "source": [
    "*Since we have seperated the currency and the actual price we can now convert all the prices to euro and then we will populate the nulls in the new column. At the end of the process we can calculate the NaN values in the column `price` as well.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ik77rvq4jnU"
   },
   "source": [
    "### Profile `df_transactions_cleaned` and summarize corrections made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(df_transations, title=\"Pandas Profiling Report\")\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*From the above tables of the profiler we can notice that there are no missing values, still some columns are high collerated.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Yiqzq3L4jnU"
   },
   "source": [
    "## Part 3: Business insights (15 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L019f5Sb4jnV"
   },
   "source": [
    "### Company by revenue\n",
    "\n",
    "The revenue of a company is its total value of orders, all time. Compute and visualise all companies by revenue in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:39:13.954452Z",
     "start_time": "2021-10-31T14:39:13.952349Z"
    },
    "id": "qS67VJG-4jnV"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "total_orders = psql.sqldf(\"\"\"\n",
    "    select company\n",
    "           ,sum(prices_euro) as revenue\n",
    "    from df_transations\n",
    "    group by company\n",
    "    order by revenue desc    \n",
    "\"\"\")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.bar(total_orders.company, \n",
    "       total_orders.revenue,\n",
    "       color=sns.color_palette(),\n",
    "      log=True)\n",
    "ax.tick_params(axis='x', rotation=75)\n",
    "fig.set_size_inches(20, 5)\n",
    "      \n",
    "plt.ylabel(\"Revenue in EUR\", size=12)\n",
    "plt.xlabel(\"Company\", size=12)\n",
    "plt.title(\"Company by revenue\", size=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VL29OwOV4jnV"
   },
   "source": [
    "### Country by revenue, per year\n",
    "\n",
    "The revenue of a country in a time period, is its total value of orders in that time period. Compute and visualise all countries by revenue, for years 2016, 2017 and 2018. Your visualisation should have countries on the x-axis and multiple bars (one for each year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:39:18.597583Z",
     "start_time": "2021-10-31T14:39:18.594419Z"
    },
    "id": "_KHGZs0O4jnV"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "revenue_year_country = psql.sqldf(\"\"\"\n",
    "    select strftime('%Y', date) as year\n",
    "        ,country\n",
    "        ,sum(prices_euro) as revenue\n",
    "    from df_transations  \n",
    "    where 1=1\n",
    "        and strftime('%Y', date) in ('2016', '2017', '2018')\n",
    "    group by country, year \n",
    "\"\"\")\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "splot = sns.barplot(x=\"country\", y=\"revenue\", hue=\"year\",\n",
    "                    data=revenue_year_country, palette=sns.color_palette(), log=True)\n",
    "\n",
    "  \n",
    "for p in splot.patches:\n",
    "    splot.annotate(format(p.get_height()),\n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                   ha='center', va='center',\n",
    "                   xytext=(0, 10))\n",
    "      \n",
    "plt.ylabel(\"Revenue in EUR\", size=12)\n",
    "plt.xlabel(\"Country\", size=12)\n",
    "plt.title(\"Revenue for country from 2016 to 2018\", size=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIwka6bY4jnV"
   },
   "source": [
    "### Orders per quarter, all companies\n",
    "\n",
    "Compute and visualise the number of orders each company has placed in each quarter. Exclude quarters where the order count is less than 3. As always, be mindful to not produce a cluttered visualisation.\n",
    "\n",
    "Part of your query should form a variable that converts `date` into `YEAR_QUARTER` format. Dealing with dates is via `STRFTIME` [docs](https://www.sqlite.org/lang_datefunc.html) which doesn't allow quarter extraction. Instead, it allows for extraction of month, which you can case on in order to produce the quarter (Q1, Q2, Q3, Q4).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:39:23.632721Z",
     "start_time": "2021-10-31T14:39:23.629720Z"
    },
    "id": "zGT4oqVl4jnV"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "quarter_orders = psql.sqldf(\"\"\"\n",
    "    select count(prices_euro) number_of_orders\n",
    "           ,company\n",
    "           ,strftime('%Y', date) || \"_\" || case \n",
    "               when strftime('%m', date) in ('01', '02', '03') then \"Q1\"\n",
    "               when  strftime('%m', date) in ('04', '05', '06') then \"Q2\"\n",
    "               when  strftime('%m', date) in ('07', '08', '09', '10') then \"Q3\"\n",
    "               else \"Q4\"\n",
    "           end as year_quarter\n",
    "           ,case \n",
    "               when strftime('%m', date) in ('01', '02', '03') then \"Q1\"\n",
    "               when  strftime('%m', date) in ('04', '05', '06') then \"Q2\"\n",
    "               when  strftime('%m', date) in ('07', '08', '09', '10') then \"Q3\"\n",
    "               else \"Q4\"\n",
    "            end as quarter\n",
    "    from df_transations \n",
    "    group by company, YEAR_QUARTER\n",
    "    having  1=1\n",
    "        and count(price) >= 3\n",
    "    order by company\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "quarter_orders\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "splot = sns.lineplot(x=\"year_quarter\", y=\"number_of_orders\", hue=\"company\",\n",
    "                    data=quarter_orders)\n",
    "\n",
    "  \n",
    "for p in splot.patches:\n",
    "    splot.annotate(format(p.get_height()),\n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                   ha='center', va='center',\n",
    "                   xytext=(0, 10))\n",
    "      \n",
    "plt.ylabel(\"Order\", size=12)\n",
    "plt.xlabel(\"Country\", size=12)\n",
    "plt.title(\"Orders per country for all the countries\", size=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xW0LZ7_b4jnV"
   },
   "source": [
    "## Part 4: Parts and prices (20 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uM6CcJQj4jnV"
   },
   "source": [
    "### Parts demand changes\n",
    "\n",
    "A different amount of orders are placed on parts each year. The demand of a part is the number of orders placed on it. The demand change of a part is the absolute difference between its average demand in 2016/2017, and its demand in 2018.\n",
    "\n",
    "Compute and visualise the 15 parts whose demand change has been the largest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:39:30.418442Z",
     "start_time": "2021-10-31T14:39:30.415270Z"
    },
    "id": "qjh6pszu4jnV"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "parts_demands = psql.sqldf(\"\"\"\n",
    "    WITH trans67 as (\n",
    "        select count(part)/2  as demand\n",
    "               ,part\n",
    "        from df_transations\n",
    "        where 1=1\n",
    "            and strftime('%Y', date) in ('2017', '2016')\n",
    "        group by part\n",
    "    )\n",
    "    select abs(count(tran18.part) - trans67.demand) demand\n",
    "           ,tran18.part\n",
    "    from df_transations tran18\n",
    "        join trans67 on trans67.part = tran18.part\n",
    "    where 1=1\n",
    "        and strftime('%Y', tran18.date) in ('2018')\n",
    "    group by tran18.part\n",
    "    order by demand desc\n",
    "    limit 15\n",
    "\"\"\")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.bar(parts_demands.part, \n",
    "       parts_demands.demand,\n",
    "       color=sns.color_palette(),\n",
    "      log=True)\n",
    "ax.tick_params(axis='x', rotation=75)\n",
    "fig.set_size_inches(20, 5)\n",
    "      \n",
    "plt.ylabel(\"Demand\", size=12)\n",
    "plt.xlabel(\"Part\", size=12)\n",
    "plt.title(\"Part and demand from 2016/2017 to 2018\", size=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNGVkZIL4jnV"
   },
   "source": [
    "### Popular parts pricing\n",
    "\n",
    "The most popular parts are those whose demand has increased the most from its 2016/2017 average to 2018. We're interested to find out if popularity is due to a price drop, and also inform us if prices of these parts are properly adjusted.\n",
    "\n",
    "The demand increase of a part is its 2018 demand minus its 2016/2017 average demand. The price change of a part is its average 2018 price minus its average 2016/2017 price.\n",
    "\n",
    "Compute the parts whose demand has increased (has positive demand increase) and the change in price for each of these parts. Then visualise this relationship and include in the figure title the correlation (compute via pandas) between these two variables. Conclude which is most likely 1) Parts became more popular from a drop in prices, or 2) The sales department deserved its bonuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:39:38.813945Z",
     "start_time": "2021-10-31T14:39:38.811979Z"
    },
    "id": "prx6EWk04jnV"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "popular_parts = psql.sqldf(\"\"\"\n",
    "    WITH trans67 as (\n",
    "        select count(part)/2  as demand\n",
    "               ,part\n",
    "               ,avg(prices_euro) as avg_price\n",
    "        from df_transations\n",
    "        where 1=1\n",
    "            and strftime('%Y', date) in ('2017', '2016')\n",
    "        group by part\n",
    "    )\n",
    "    select (count(tran18.part) - trans67.demand) as demand -- if positve demand inc\n",
    "           ,tran18.part\n",
    "           ,(avg(tran18.prices_euro) - trans67.avg_price) as avg_price\n",
    "    from df_transations tran18\n",
    "        join trans67 on trans67.part = tran18.part\n",
    "    where 1=1\n",
    "        and strftime('%Y', tran18.date) in ('2018')\n",
    "    group by tran18.part\n",
    "    having (count(tran18.part) - trans67.demand) > 0\n",
    "\"\"\")\n",
    "\n",
    "corr = popular_parts.demand.corr(popular_parts.avg_price)\n",
    "\n",
    "txt_title = 'Demand with average price, left y-xis = Demand between the 2016/2017 and 2018, right y-axis =\\\n",
    "avgPrice between the 2016/2017 and 2018, correlation between these columns is: ' + str(corr)\n",
    "\n",
    "popular_parts\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(25, 10))\n",
    "_= popular_parts.plot(ax= ax , kind=\"bar\", x='part', y='demand', legend=False)\n",
    "ax2 = ax.twinx()\n",
    "_ = popular_parts.plot(ax= ax2 , kind=\"line\", x='part', color=\"red\", alpha=.6, y='avg_price', legend=False)\n",
    "ax2.title.set_text(txt_title)\n",
    "legend = ax2.legend(['Avg price in thousands'], loc='upper right', shadow=True, fontsize='x-large')\n",
    "legend = ax.legend([\"Demand between years\"], loc='upper left', shadow=True, fontsize='x-large')\n",
    "\n",
    "# Put a nicer background color on the legend.\n",
    "legend.get_frame().set_facecolor('C0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTYM02c14jnW"
   },
   "source": [
    "*From the plot it can be noticed that some of the parts for instane 49520-501 the demand increased but also the price was increased, more specific it is the largest price different between the 2016/2017 and 2018. So good job salesperson! In contrast for the second product 0699-7041 it seems that the demand was increased due to the price drop.*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "02807_Project_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "432.448px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
